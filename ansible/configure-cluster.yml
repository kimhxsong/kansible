---
# 1단계: 모든 노드 공통 설정
- name: Configure Kubernetes cluster prerequisites
  hosts: k8s_cluster
  become: yes
  tasks:
    # 공통 설정 (모든 노드)
    - name: Update /etc/hosts with all cluster nodes
      blockinfile:
        path: /etc/hosts
        backup: yes
        marker: "# {mark} ANSIBLE MANAGED BLOCK - K8S CLUSTER"
        block: |
          {% for host in groups['k8s_cluster'] %}
          {{ hostvars[host]['ansible_host'] }} {{ host }}
          {% endfor %}

    - name: Set hostname based on inventory
      hostname:
        name: "{{ inventory_hostname }}"

    # Swap 비활성화 (모든 노드 필수)
    - name: Disable swap
      shell: swapoff -a

    - name: Disable swap permanently
      replace:
        path: /etc/fstab
        regexp: '^(\s*)([^#\n]+\s+)(\w+\s+)swap(\s+.*)$'
        replace: '#\1\2\3swap\4'
        backup: yes

    # 방화벽 비활성화 (모든 노드)
    - name: Disable UFW firewall
      ufw:
        state: disabled

    # 기본 패키지 설치 (모든 노드)
    - name: Install basic dependencies
      apt:
        name:
          - curl
          - ca-certificates
          - gnupg
          - lsb-release
          - apt-transport-https
        state: present
        update_cache: yes

    # 컨테이너 런타임 설치 (모든 노드 필수)
    - name: Install containerd
      apt:
        name: containerd
        state: present
        update_cache: no

    - name: Create containerd config directory
      file:
        path: /etc/containerd
        state: directory

    - name: Generate containerd config
      shell: containerd config default > /etc/containerd/config.toml

    - name: Configure containerd to use systemd cgroup driver
      replace:
        path: /etc/containerd/config.toml
        regexp: "SystemdCgroup = false"
        replace: "SystemdCgroup = true"

    - name: Restart and enable containerd
      systemd:
        name: containerd
        state: restarted
        enabled: yes

    # 커널 모듈 및 시스템 설정 (모든 노드)
    - name: Load kernel modules
      modprobe:
        name: "{{ item }}"
      loop:
        - br_netfilter
        - overlay

    - name: Make kernel modules persistent
      copy:
        content: |
          br_netfilter
          overlay
        dest: /etc/modules-load.d/k8s.conf

    - name: Configure sysctl for Kubernetes
      copy:
        content: |
          net.bridge.bridge-nf-call-iptables  = 1
          net.bridge.bridge-nf-call-ip6tables = 1
          net.ipv4.ip_forward                 = 1
        dest: /etc/sysctl.d/k8s.conf

    - name: Apply sysctl settings
      shell: sysctl --system

    # 시스템 아키텍처 확인
    - name: Detect system architecture
      shell: uname -m
      register: system_arch
      changed_when: false

    - name: Create CNI bin directory
      file:
        path: /opt/cni/bin
        state: directory
        mode: "0755"

    - name: Set architecture variable
      set_fact:
        k8s_arch: "{{ 'arm64' if system_arch.stdout in ['aarch64', 'arm64'] else 'amd64' }}"

    # Kubernetes 버전 확인 (모든 노드)
    - name: Get latest kubectl version
      shell: curl -L -s https://dl.k8s.io/release/stable.txt
      register: kubectl_version

    # 공통 Kubernetes 바이너리 설치 (모든 노드)
    - name: Download kubelet binary (required on all nodes)
      get_url:
        url: "https://dl.k8s.io/release/{{ kubectl_version.stdout }}/bin/linux/{{ k8s_arch }}/kubelet"
        dest: /usr/local/bin/kubelet
        mode: "0755"

    - name: Download kubeadm binary (required on all nodes)
      get_url:
        url: "https://dl.k8s.io/release/{{ kubectl_version.stdout }}/bin/linux/{{ k8s_arch }}/kubeadm"
        dest: /usr/local/bin/kubeadm
        mode: "0755"

    - name: Download kubectl binary (master + optional for workers)
      get_url:
        url: "https://dl.k8s.io/release/{{ kubectl_version.stdout }}/bin/linux/{{ k8s_arch }}/kubectl"
        dest: /usr/local/bin/kubectl
        mode: "0755"

    # kubelet 서비스 설정 (모든 노드)
    - name: Create kubelet systemd service
      copy:
        content: |
          [Unit]
          Description=kubelet: The Kubernetes Node Agent
          Documentation=https://kubernetes.io/docs/home/
          Wants=network-online.target
          After=network-online.target

          [Service]
          ExecStart=/usr/local/bin/kubelet
          Restart=always
          StartLimitInterval=0
          RestartSec=10

          [Install]
          WantedBy=multi-user.target
        dest: /etc/systemd/system/kubelet.service

    - name: Create kubelet service directory
      file:
        path: /etc/systemd/system/kubelet.service.d
        state: directory

    - name: Create kubelet service override
      copy:
        content: |
          [Service]
          Environment="KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf"
          Environment="KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml"
          EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
          EnvironmentFile=-/etc/default/kubelet
          ExecStart=
          ExecStart=/usr/local/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS
        dest: /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

    - name: Reload systemd and enable kubelet
      systemd:
        name: kubelet
        enabled: yes
        daemon_reload: yes

    # 버전 확인
    - name: Check installed versions
      shell: |
        echo "kubelet: $(kubelet --version)"
        echo "kubeadm: $(kubeadm version --output=short)"
        echo "kubectl: $(kubectl version --client --output=yaml | grep gitVersion)"
      register: version_check
      changed_when: false

    - name: Display installed versions
      debug:
        msg: "{{ version_check.stdout_lines }}"

# 2단계: 마스터 노드 초기화
- name: Initialize Kubernetes master
  hosts: k8s-master
  become: yes
  tasks:
    - name: Check if cluster is already initialized
      stat:
        path: /etc/kubernetes/admin.conf
      register: cluster_initialized

    - name: Check if cluster is healthy (if already initialized)
      shell: kubectl get nodes
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: cluster_health
      ignore_errors: yes
      when: cluster_initialized.stat.exists

    - name: Display current cluster status
      debug:
        msg: |
          Cluster status: {{ 'Already initialized' if cluster_initialized.stat.exists else 'Not initialized' }}
          {% if cluster_initialized.stat.exists and cluster_health.rc == 0 %}
          Current nodes:
          {{ cluster_health.stdout }}
          {% endif %}

    - name: Set cluster reset needed flag
      set_fact:
        cluster_needs_reset: "{{ cluster_initialized.stat.exists and cluster_health.rc != 0 }}"

    - name: Reset existing cluster if needed
      block:
        - name: Confirm cluster reset
          pause:
            prompt: |
              ⚠️  Kubernetes cluster already exists but may be broken.
              Do you want to reset and reinitialize? (yes/no)
            echo: yes
          register: reset_confirm

        - name: Reset cluster
          shell: kubeadm reset -f
          when: reset_confirm.user_input == 'yes'

        - name: Clean up old cluster files
          file:
            path: "{{ item }}"
            state: absent
          loop:
            - /etc/kubernetes
            - /var/lib/kubelet
            - /var/lib/etcd
            - /etc/cni/net.d
            - /root/.kube
            - /home/vagrant/.kube
          when: reset_confirm.user_input == 'yes'

        - name: Restart kubelet
          systemd:
            name: kubelet
            state: restarted
          when: reset_confirm.user_input == 'yes'

        - name: Update cluster initialization status
          set_fact:
            cluster_initialized: { stat: { exists: false } }
          when: reset_confirm.user_input == 'yes'
      when: cluster_needs_reset

    - name: Set cluster initialization needed flag
      set_fact:
        cluster_needs_init: "{{ not cluster_initialized.stat.exists or (cluster_health is defined and cluster_health.rc != 0) }}"

    - name: Initialize cluster
      shell: |
        kubeadm init \
          --apiserver-advertise-address={{ hostvars['k8s-master']['ansible_host'] }} \
          --pod-network-cidr=10.244.0.0/16 \
          --service-cidr=10.96.0.0/12
      register: kubeadm_init
      when: cluster_needs_init

    - name: Setup kubectl for root
      shell: |
        mkdir -p /root/.kube
        cp /etc/kubernetes/admin.conf /root/.kube/config
      when: cluster_needs_init

    - name: Setup kubectl for vagrant user
      shell: |
        mkdir -p /home/vagrant/.kube
        cp /etc/kubernetes/admin.conf /home/vagrant/.kube/config
        chown vagrant:vagrant /home/vagrant/.kube/config
      when: cluster_needs_init

    - name: Install CNI plugins (required before Flannel)
      shell: |
        export CNI_VERSION="v1.3.0"
        curl -L "https://github.com/containernetworking/plugins/releases/download/${CNI_VERSION}/cni-plugins-linux-{{ k8s_arch }}-${CNI_VERSION}.tgz" | tar -C /opt/cni/bin -xz
        chmod +x /opt/cni/bin/*
      when: cluster_needs_init

    - name: Check if Flannel is already installed
      shell: kubectl get pods -n kube-system | grep flannel
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: flannel_check
      ignore_errors: yes

    - name: Install CNI (Flannel) if not already installed
      shell: kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      when: flannel_check.rc != 0

    - name: Wait for master node to be Ready
      shell: kubectl get node {{ ansible_hostname }} --no-headers | awk '{print $2}'
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: master_status
      until: master_status.stdout == "Ready"
      retries: 30
      delay: 10

    - name: Display master initialization result
      debug:
        msg: |
          ✅ Master node {{ ansible_hostname }} is ready!
          Master status: {{ master_status.stdout }}
          {% if cluster_needs_init %}
          New cluster initialized successfully!
          {% else %}
          Using existing healthy cluster.
          {% endif %}

# 3단계: 워커 노드 조인
- name: Join worker nodes to cluster
  hosts: k8s-worker*
  become: yes
  #serial: 1 # 한 번에 하나씩 안전하게 처리
  vars:
    master_node: k8s-master
  tasks:
    - name: Check if node already exists in cluster
      shell: kubectl get node {{ ansible_hostname }}
      delegate_to: "{{ master_node }}"
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: node_exists
      ignore_errors: yes
      failed_when: false

    - name: Set node join needed flag
      set_fact:
        node_needs_join: "{{ node_exists is not defined or node_exists.rc != 0 }}"

    - name: Skip if already joined
      debug:
        msg: "✅ Node {{ ansible_hostname }} already exists in cluster, skipping join"
      when: not node_needs_join

    - name: Join worker to cluster
      block:
        - name: Generate join command
          shell: kubeadm token create --print-join-command
          delegate_to: "{{ master_node }}"
          register: join_command

        - name: Execute join
          shell: "{{ join_command.stdout }}"
          register: join_result

        - name: Verify join success
          debug:
            msg: |
              ✅ Node {{ ansible_hostname }} successfully joined:
              {{ join_result.stdout }}

        - name: Wait for node to be Ready
          shell: kubectl get node {{ ansible_hostname }} --no-headers | awk '{print $2}'
          delegate_to: "{{ master_node }}"
          environment:
            KUBECONFIG: /etc/kubernetes/admin.conf
          register: node_ready
          until: node_ready.stdout == "Ready"
          retries: 20
          delay: 15

        - name: Setup kubectl for vagrant user on worker (optional)
          shell: |
            mkdir -p /home/vagrant/.kube
            scp -o StrictHostKeyChecking=no vagrant@{{ hostvars['k8s-master']['ansible_host'] }}:/etc/kubernetes/admin.conf /home/vagrant/.kube/config
            chown vagrant:vagrant /home/vagrant/.kube/config
            chmod 644 /home/vagrant/.kube/config
          ignore_errors: yes
          become: yes

        - name: Display node join status
          debug:
            msg: |
              ✅ Worker node {{ ansible_hostname }} successfully joined and ready!
              Node status: {{ node_ready.stdout }}

      when: node_needs_join

# 4단계: 클러스터 검증
- name: Validate cluster deployment
  hosts: k8s-master
  become: yes
  tasks:
    - name: Get cluster info
      shell: kubectl cluster-info
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: cluster_info

    - name: Get all nodes
      shell: kubectl get nodes -o wide
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: all_nodes

    - name: Get system pods
      shell: kubectl get pods -n kube-system
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: system_pods

    - name: Check cluster health
      shell: |
        # 모든 노드가 Ready인지 확인
        NOT_READY=$(kubectl get nodes --no-headers | grep -v " Ready " | wc -l)

        # 시스템 포드가 모두 Running인지 확인  
        NOT_RUNNING=$(kubectl get pods -n kube-system --no-headers | grep -v "Running\|Completed" | wc -l)

        echo "Nodes not ready: $NOT_READY"
        echo "Pods not running: $NOT_RUNNING"

        if [ $NOT_READY -eq 0 ] && [ $NOT_RUNNING -eq 0 ]; then
          echo "CLUSTER_STATUS: HEALTHY"
        else
          echo "CLUSTER_STATUS: UNHEALTHY"
        fi
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: health_check

    - name: Display cluster status
      debug:
        msg: |
          🎯 Kubernetes Cluster Deployment Complete!

          === CLUSTER INFO ===
          {{ cluster_info.stdout }}

          === NODES ===
          {{ all_nodes.stdout }}

          === SYSTEM PODS ===
          {{ system_pods.stdout }}

          === HEALTH CHECK ===
          {{ health_check.stdout }}

          {% if 'HEALTHY' in health_check.stdout %}
          ✅ Cluster is healthy and ready for workloads!

          Next steps:
          1. Deploy applications: kubectl create deployment nginx --image=nginx
          2. Create services: kubectl expose deployment nginx --port=80 --type=NodePort
          3. Check with: kubectl get all
          {% else %}
          ⚠️  Cluster has some issues. Check individual components.
          {% endif %}
